{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initiation to Word Embedding\n",
    "\n",
    "## Julien Velcin\n",
    "\n",
    "<div style=\"margin-left: 10px;\">Laboratoire ERIC</div>\n",
    "\n",
    "<div style=\"margin-left: 10px;\">Université Lyon 2</div>\n",
    "\n",
    "[@jvelcin](https://twitter.com/jvelcin)\n",
    "\n",
    "[http://mediamining.univ-lyon2.fr/velcin/](http://mediamining.univ-lyon2.fr/velcin/)\n",
    "\n",
    "*Notes initially destined to the students attending the DMKM Master *\n",
    "\n",
    "*Revised for a talk given in Apr. 2016 to the data mining and decision group of the ERIC Lab*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common background and the distributional hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Famous quote of the linguist John Rupert Firth (1957):\n",
    "\n",
    "<div style=\"margin-left: 30px;\">\"You shall know a word by the company it keeps\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Like in:\n",
    ">\"I drink coffee\"  \n",
    ">\"He drinks milk\"  \n",
    ">\"Mary sips tea\"  \n",
    ">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "words drink and sip, also form paradigmatic classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And collocations:\n",
    "    \n",
    ">\"New York City\"  \n",
    ">\"black and white\"  \n",
    ">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distributional hypothesis (Harris, 1954):\n",
    "\n",
    "\"words that occur in the same contexts tend to have similar meanings\"\n",
    "\n",
    "<center>\n",
    "  <img src='img/distrib_example.png' style='height: 200px'>\n",
    "</center>\n",
    "\n",
    "*Harris, Z. (1954), Distributional structure. Word, 10(23), pp. 146-162.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take again the examples (thanks to A. Toth for the illustration):\n",
    "\n",
    "<table style=\"border:0;\">\n",
    "<tr style=\"border: 0;\">\n",
    "<td style=\"border: 0; white-space:pre; padding:0 100px 0 0px;\">\n",
    "\"I drink coffee\"\n",
    "\"He drinks milk\"\n",
    "\"Mary sips tea\"  \n",
    "</td>\n",
    "<td style=\"border: 0; \">\n",
    "  <img src='img/context_matrix.png' style='height: 300px'>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It means that a word can be \"represented\" by an **histogram** over a given vocabulary, its **context**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is *not* Word Embedding..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"margin-left: 30px;\">...computing term co-occurrences</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a href=\"img/HP_cooc.jpg\"><img src='img/HP_cooc.jpg' style='height: 400px'></a>\n",
    "\n",
    "(produced with t-SNE, package Rtsne with Barnes-Hut implementation)\n",
    "\n",
    "*Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9, pp. 2579-2605*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"margin-left: 30px;\">...topic modeling</div>\n",
    "\n",
    "<img src='img/topicmodels.png' style='height: 300px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can be viewed as a *specific case*, in which context = documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main ideas to take home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Distributional semantics**\n",
    "\n",
    "<img src='img/distribut.png' style='height: 300px'>\n",
    "\n",
    "[taken from the talk of I. Sutskever at UC Berkeley in Feb. 2014](https://archive.org/details/Redwood_Center_2014_02_12_Ilya_Sutskever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Represent a word with the distribution of its neighbors\n",
    "\n",
    "Similar words have similar sets of neighbors:\n",
    "\n",
    "    \"I took my *dog* for a walk.\"\n",
    "    \"I took my *cat* for a walk.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, the words *dog* and *cat* may be switched (*exchangeability* hypothesis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Important **underlying hypotheses** (Turney and Pantel, 2010):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Statistical semantics hypothesis**: patterns of human word usage ~ meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Bag of words hypothesis**: frequency of words ~ relevance to a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Distributional hypothesis**: similar context ~ similar meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **two remaining hypotheses** are related to pair–pattern matrices (more suited to *relational* similarity, see Gentner 1983). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Histograms of words have nice, suprising capabilities:\n",
    "\n",
    "- capture semantics\n",
    "- let us work in a vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But...\n",
    "\n",
    "- highly dimensional\n",
    "- bad estimation of rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution is to **reduce the space dimensionality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different approaches\n",
    "\n",
    "Three classes of VSMs (Turney and Pantel, 2010):\n",
    "- based on term–document\n",
    "- based on word–context\n",
    "- based on pair–pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Count vs. predict (Baroni et al., 2014):\n",
    "- Counting (traditional) approaches\n",
    "- Predictive (new) approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Importance of the **context** on how you build your co-occurrence matrix:\n",
    "\n",
    "- Globally: context of a word is the **set of documents**\n",
    "- Locally: context of  word is a sliding window of **co-occurring words** (or more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The local approach seems to better capture fine-grained syntactical and semantical relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An interesting issue lies in **reconcilating the two views**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predictive approaches for DSMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Three main ideas in the nutshell (derived from Bengio et al., 2003):\n",
    "    \n",
    "- associate with each word in the vocabulary a distributed word feature vector (a real-valued vector in $\\mathbb{R}^m$),\n",
    "- design a model to solve a NLP task based on these feature vectors\n",
    "- learn simultaneously the word feature vectors and the parameters of the model.\n",
    "\n",
    "<br/>\n",
    "*Bengio, Y., Ducharme, R., Vincent P. and Jauvin C. (2003), A Neural probabilistic language models. Journal of Machine Learning Research, vol.3, pp. 1137-1155.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- associate with each word in the vocabulary a distributed word feature vector (a real-valued vector in $\\mathbb{R}^m$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"margin-left: 30px;\">W(\"drink\") = (0.2, 0.56, -0.12... 0.65)</div>\n",
    "\n",
    "<div style=\"margin-left: 30px;\">W(\"coffee\") = (-0.3, 0.1, -0.5... 0.06)</div>\n",
    "\n",
    "<div style=\"margin-left: 30px;\">W(\"tea\") = (-0.27, 0.09, -0.5... 0.03)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- design a model to solve a NLP task based on these feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='img/ex_bottou.png' style='height: 200px'></center>\n",
    "\n",
    "*Bottou, L. (2014). From machine learning to machine reasoning. Machine learning, 94(2), pp. 133-149*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- learn **simultaneously** the word feature vectors W and the parameters of the model R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, really, we only care about W..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a href=\"img/ex_embedding.png\"><img src='img/ex_embedding.png' style='height: 400px'></a>\n",
    "\n",
    "*Turian, J., Ratinov, L., & Bengio, Y. (2010). Word representations: a simple and general method for semi-supervised learning. Proceedings of the 48th ACL, pp. 384-394.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And if we look at the neighborhood of some words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='img/ex_embedding_2.png' style='height: 250px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Observations on embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Different kinds of embedded relations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='img/diff_relations.png' style='height: 400px'>\n",
    "\n",
    "<br/>\n",
    "*Mikolov, T., Yih, W. T., & Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Proceedings of HLT-NAACL, pp. 746-751.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solving analogies:\n",
    "\n",
    "<img src='img/analogies.png' style='height: 250px'>\n",
    "\n",
    "<br/>\n",
    "*Mikolov, T., Yih, W. T., & Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Proceedings of HLT-NAACL, pp. 746-751.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Embeddings are usefull for many NLP tasks:\n",
    "\n",
    "*\"The use of word representations… has become a key “secret sauce” for the success of many NLP systems in recent years, across tasks including named entity recognition, part-of-speech tagging, parsing, and semantic role labeling.\"* (Luong et al., 2013)\n",
    "\n",
    "<br/>\n",
    "*Luong, T., Socher, R., & Manning, C. D. (2013). Better Word Representations with Recursive Neural Networks for Morphology. Proceedings of CoNLL, pp. 104-113.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Various NN architectures for Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bengio et al., 2003\n",
    "<img src='img/arch1.png' style='height: 500px'>\n",
    "\n",
    "*Bengio, Y., Ducharme, R., Vincent P. and Jauvin C. (2003), A Neural probabilistic language models. Journal of Machine Learning Research, vol.3, pp. 1137-1155.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bengio et al., 2003\n",
    "**Task:** predict the next term by maximizing $\\hat{P}(w_t|w_{t-n+1}^{t−1})$ given the (n-1) previous words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\hat{P}(w_t|w_{t-n+1}^{t−1}) = \\frac{e^{y_{wt}}}{\\sum_i e^{y_i}}$ (softmax)\n",
    "\n",
    "with:\n",
    "\n",
    "* $\\textbf{y} = b + W\\textbf{x} + U\\tanh(d+H\\textbf{x})$\n",
    "* $b$ and $d$ as biases (usual in MLP)\n",
    "* $W$, $H$ and $U$ as weight matrices between layers\n",
    "* $\\textbf{x} = (C(w_{t−1}),C(w_{t−2})$... $, C(w_{t-n+1}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$C$ is precisely the function mapping $w_i$ to the word features $C(w_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Learning took 3 weeks with 40 CPUs on the Brown corpus, vocabulary of 17,964 words and 30 to 100 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Collobert et al., 2011\n",
    "<img src='img/arch2.png' style='height: 500px'>\n",
    "\n",
    "*Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12, pp. 2493-2537.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Collobert et al., 2011\n",
    "\n",
    "**Task:** solve one NLP task\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>POS</td><td>CHUNK</td><td>NER</td><td>SRL</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Part of speech</td><td>Chunking</td><td>Named entities recognition</td><td>Semantic role labeling</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Coping with this task needs to embed the terms into a multidimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, the targeted term $t$ will be described by using its $d_win$ surrounding words (**window approach network**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Actually, the same authors propose another architecture for dealing with the whole sentence by using a convolutional network (**sentence approach network**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most important part for us: the lookup table\n",
    "\n",
    "$LT_{W^1,\\ldots W^K}([w]_1^T) =\n",
    "\\begin{pmatrix}\n",
    "\\langle W^1 \\rangle^1_{[w_1]_1} & \\ldots & \\langle W^1 \\rangle^1_{[w_1]_T} \\\\\n",
    "\\vdots & & \\vdots\\\\\n",
    "\\langle W^K \\rangle^1_{[w_K]_1} & \\ldots & \\langle W^K \\rangle^1_{[w_K]_T} \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "where:\n",
    "\n",
    "* $[w_p]_q$ is the $p$th feature describing the $q$th word\n",
    "* $\\langle W^p \\rangle$ is the features vector related to the $p$th feature\n",
    "\n",
    "Actually, a single word can be associated to several raw input features:\n",
    "\n",
    "*singing* $\\longleftrightarrow$ { sing (stemmed root), ing (ending), no (capitalization) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The feature matrix LT is mapped to a feature vector by stacking its columns up (\"concat\"):\n",
    "\n",
    "$f_\\theta^1 = < LT_W([w]_1^T) >_t^{d_{win}} =\n",
    "\\begin{pmatrix}\n",
    "\\langle W \\rangle^1_{[m]_{t-d_{win}/2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\langle W \\rangle^1_{[m]_{t}} \\\\\n",
    "\\vdots \\\\\n",
    "\\langle W \\rangle^1_{[m]_{t+d_{win}/2}} \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$f_\\theta^1$ is then fed to the next layers of the network:\n",
    "<img src='img/arch2.png' style='height: 500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training is performed by maximizing a likelihood score over training data by using **stochastic gradient ascent** and backpropagating the observed error as in usual MLP.\n",
    "\n",
    "Two trick are used for improving initialization and parameter update, based on a \"fan-in\" score estimated for each layer (Plaut and Hinton, 1987).\n",
    "\n",
    "Based on a vocabulary of the 10,000 most frequent words, training time differs regarding the task (from one hour for NER to three days for SRL).\n",
    "\n",
    "And the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='img/res_collobert.png' style='height: 150px'>\n",
    "WLL = word-level log-likelihood / SLL = sentence-level log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To sum up:\n",
    "\n",
    "* an architecture *not* dedicated to a particular NLP task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* that can be improved in several ways, in particular by using unsupervised data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* that provide us (much) better embeddings:\n",
    "\n",
    "<img src='img/collobert_embedding.png' style='height: 250px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mikolov et al., 2013\n",
    "<img src='img/arch3.png' style='height: 400px'>\n",
    "\n",
    "*Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mikolov et al., 2013\n",
    "**Task:** Learn the word embeddings by solving a specific NLP prediction task.\n",
    "\n",
    "* Continous BOW model (**CBOW**):  predict $w_t$ given the neigborhood words $w_{t-n+1}, w_{t-n+2}\\ldots , w_{t+n-2}, w_{t-n+1}$\n",
    "\n",
    "* Continuous Skip-gram model (**Skip-gram**): predict the neigborhood words $w_{t-n+1}, w_{t-n+2}\\ldots , w_{t+n-2}, w_{t-n+1}$ given $w_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training is performed by using **backpropagation** and **stochastic gradient** on a Google News corpus of 6B tokens associated to 1 million words.\n",
    "\n",
    "The tested feature dimensionality ranges from 50 to 1000 (previsouly, about 50-100).\n",
    "\n",
    "Learning took several days with multiple CPUs, with a final accuracy up to 65%.\n",
    "\n",
    "In order to address the computational runtime issue, the **hierarchical softmax** extent has been proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples of learned relations with the Skip-gram model (trained on 783M words with 300 dimensionality):\n",
    "<img src='img/ex_embedding_skipgram.png' style='height: 400px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Possible link between WE and topic models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Comparison between WE and topic learning models (e.g., SVD and NMF)\n",
    "\n",
    "*Baroni, M., Dinu, G., & Kruszewski, G. (2014, June). Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL (1) (pp. 238-247).*\n",
    "\n",
    "* Mixing WE and topic models\n",
    "\n",
    "*Das, R., Zaheer, M., & Dyer, C. (2015). Gaussian LDA for topic models with word embeddings. In Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='img/gaussian_LDA.png' style='height: 700px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "1- Word embedding = learning distributed representation of words\n",
    "\n",
    "2- Word embedding $\\neq$ deep learning $\\neq$ topic modeling (but bridges can be built)\n",
    "\n",
    "3- Word embedding as a lever to solve NLP issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Open issues\n",
    "\n",
    "Same linguistic regularities captured by WE are also captured by explicit count-based models (Levy and Goldberg, 2014).\n",
    "\n",
    "If carefully tuned, no significant difference observed in the performance between count and prediction-based models (Levy et al., 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Distributional Hypothesis is a claim about semantic similarity, which is a really **vague notion** (Fabre and Lenci, 2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "<div style=\"font-size:90%\">\n",
    "Bengio, Y., Ducharme, R., Vincent P. and Jauvin C. (2003), A Neural probabilistic language models. Journal of Machine Learning Research, vol.3, pp. 1137-1155.\n",
    "[link](http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/BengioDucharmeVincentJanvin2003.pdf)\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size:90%;\">\n",
    "Bottou, L. (2014). From machine learning to machine reasoning. Machine learning, 94(2), pp. 133-149\n",
    "[link](http://research.microsoft.com/pubs/192773/tr-2011-02-08.pdf)\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size:90%;\">\n",
    "Colah's blog (2014). Deep Learning, NLP, and Representations.\n",
    "[link](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size:90%;\">\n",
    "Fabre, C. and Lenci, A. (2015). Distributional Semantics Today Introduction to the special issue. Traitement Automatique des Langues, Lavoisier. Sémantique distributionnelle, 56 (2), pp.7-20.\n",
    "[link](https://hal.archives-ouvertes.fr/hal-01259695/document)\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size:90%;\">\n",
    "Levy O., Goldberg Y. and Dagan I. (2015), Improving Distributional Similarity with Lessons Learned from Word Embeddings, Transactions of the ACL, vol. 3, p. 211-225.\n",
    "[link](https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiz0ubkotTLAhUFrxoKHRzbDQ4QFggfMAA&url=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FQ15-1016&usg=AFQjCNFX5q7FbdQrn7_tu4_XC3Z0pvPySw&sig2=emSJPhARDYriGwB1Wts93Q&bvm=bv.117218890,d.d2s)\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size:90%;\">\n",
    "Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n",
    "[link](http://arxiv.org/pdf/1301.3781.pdf)\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size:90%;\">\n",
    "Potts, C. (2013). Distributional approaches to word meanings. Ling 236/Psych 236c: Representations of meaning.\n",
    "[link](http://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf)\n",
    "</div>\n",
    "\n",
    "<div style=\"font-size:90%;\">\n",
    "Turney P. D., Pantel P. (2010). From frequency to meaning: Vector space models of semantics, Journal of artificial intelligence research, vol. 37, no 1, p. 141-188.\n",
    "[link](https://www.jair.org/media/2934/live-2934-4846-jair.pdf)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "Bengio, Y., Ducharme, R., Vincent P. and Jauvin C. (2003), A Neural probabilistic language models. Journal of Machine Learning Research, vol.3, pp. 1137-1155.\n",
    "[link](http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/BengioDucharmeVincentJanvin2003.pdf)\n",
    "\n",
    "Bottou, L. (2014). From machine learning to machine reasoning. Machine learning, 94(2), pp. 133-149\n",
    "[link](http://research.microsoft.com/pubs/192773/tr-2011-02-08.pdf)\n",
    "\n",
    "Colah's blog (2014). Deep Learning, NLP, and Representations.\n",
    "[link](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n",
    "\n",
    "Fabre, C. and Lenci, A. (2015). Distributional Semantics Today Introduction to the special issue. Traitement Automatique des Langues, Lavoisier. Sémantique distributionnelle, 56 (2), pp.7-20.\n",
    "[link](https://hal.archives-ouvertes.fr/hal-01259695/document)\n",
    "\n",
    "Levy O., Goldberg Y. and Dagan I. (2015), Improving Distributional Similarity with Lessons Learned from Word Embeddings, Transactions of the ACL, vol. 3, p. 211-225.\n",
    "[link](https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiz0ubkotTLAhUFrxoKHRzbDQ4QFggfMAA&url=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FQ15-1016&usg=AFQjCNFX5q7FbdQrn7_tu4_XC3Z0pvPySw&sig2=emSJPhARDYriGwB1Wts93Q&bvm=bv.117218890,d.d2s)\n",
    "\n",
    "Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n",
    "[link](http://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "Potts, C. (2013). Distributional approaches to word meanings. Ling 236/Psych 236c: Representations of meaning.\n",
    "[link](http://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf)\n",
    "\n",
    "Turney P. D., Pantel P. (2010). From frequency to meaning: Vector space models of semantics, Journal of artificial intelligence research, vol. 37, no 1, p. 141-188.\n",
    "[link](https://www.jair.org/media/2934/live-2934-4846-jair.pdf)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
